AskUbuntu	1
Accuracy	2
A novel transductive sequence-to-graph parsing model that jointly learns to map natural language uttearcnes to UDS graph structures and predict decompositional semantic attribute scores.	3
Modality-transferable model with cross-modality emotion embeddings	4
Viktor  Varga 	5
Hanna  Nurmi 	6
Miguel  Balles- Teros 	7
Semantic parsing of English sentences into graph-based semantic representations	8
Veselin  Stoyanov 	9
QWK	10
Current models fail to exploit emotion relationships fully and struggle with low-resource scenarios, especially for unseen emotions.	11
Masayuki  Asahara 	12
GPT-too: A Language-Model-First Approach for AMR-to-Text Generation	13
UN Parallel Corpus	14
Shanghai University of Finance and Economics	15
Jan  Guillaume 	16
Department of Computer Science	17
Jax  ùëé 	18
Chenliang  Li 	19
University of Rochester	20
To solve the slow parsing speed caused by type constraints in AM dependency parsing, the introduction of a transition-based parser, along with an A* parser, dramatically improves speed up to three orders of magnitude while ensuring well-typedness and maintaining or improving accuracy.	21
XLM-R	22
OC	23
Jenna  Kanayama 	24
En-De	25
Current methods require language-specific labelled data, which limits evaluation and application in multilingual and low-resource settings.	26
Hermann  Ney 	27
Yusuke  Mititelu 	28
Kepa  Bengoetxea 	29
Jan  Alonso 	30
Quora Question Pairs	31
The study proposes multilingual sentence embedding models based on a multi-task trained dual-encoder learning tied cross-lingual representations through translation bridge tasks.	32
To solve the issues of static word embeddings and lack of explainability in review-based recommender systems, the innovative BENEFICT model is proposed, integrating BERT, MLP, and MSP. The results show consistent outperforming performance (7% gain) and improved explanation quality (QWK 0.2019 and 0.47).	33
Weiyue  Wang 	34
Parsing natural language into Universal Decompositional Semantics	35
Sheng  Zhang 	36
['Label Smoothing', 'Neural Machine Translation', 'Generalization']	37
Smatch precision	38
W-Acc	39
To solve the issues of sub-optimal performance due to under-utilized emotion relationships and challenges with low-resource emotions in emotion recognition tasks, the modality-transferable model with emotion embeddings is innovatively proposed and achieves state-of-the-art results, particularly demonstrating its effectiveness in zero-shot and few-shot learning scenarios.	40
Petya  Osenova 	41
Young-Suk  Lee 	42
University of Sheffield	43
Wolf- Gang  Seeker 	44
Yoav  Gojenola 	45
Martin  Popel 	46
['AM Dependency Parsing', 'Semantic Parsing', 'Linguistic Typing']	47
chrF++	48
A filtering approach using multilingual BERT for sentence parallelism and GPT as a domain filter to improve corpus quality.	49
Reinald Adrian Pugoy 	50
Filip  Galbraith 	51
The paper introduces a generalized formula and theoretical solution for label smoothing, combined with empirical studies on varying tokens, probability masses, and prior distributions.	52
Furu  Wei 	53
Human Language Technology and Pattern Recognition Group Computer Science Department	54
['Simultaneous Neural Machine Translation', 'Prefix Translation', 'Stopping Criterion']	55
Radu  Simionescu 	56
Tiezheng  Yu 	57
Slav  Petrov 	58
Teresa  Ljube≈°iƒá 	59
Sampo  Pyysalo 	60
CMU-MOSEI	61
To solve the problem of requiring language-specific labelled data in quality estimation for machine translation, the innovative multilingual and zero-shot QE models are proposed, achieving a Pearson correlation of up to 0.79 on the WMT 2020 QE Task 1 dataset.	62
Korean NLP tasks	63
Institute for Natural Language Processing	64
XLM-R, a Transformer-based multilingual masked language model pre-trained on text from 100 languages.	65
Sangwhan  Moon 	66
Rating AMR graph quality	67
Naman  Goyal 	68
Benjamin  Van Durme 	69
Sebastian  Schuster 	70
Proceedings of the 12th Language Resources and Evaluation Conference	71
['Multilingual NLP', 'Vocabulary Size', 'Byte-Pair Encoding']	72
Jen- Nifer  Farkas 	73
Ajay  Nagesh 	74
Existing methods for multilingual sentence embeddings do not achieve optimal performance across a variety of semantic retrieval and related tasks.	75
To solve the large vocabulary size problem existing in multilingual NLP models, two new algorithms are innovatively proposed, leading to a significant reduction in the vocabulary budget required for Korean, achieving almost negligible OOV rates with smaller vocabulary sizes.'	76
Ne-En	77
BERT-Based Neural Collaborative Filtering and Fixed-Length Contiguous Tokens Explanation	78
Microsoft Research Asia	79
Chris Tar ùëé 	80
Alessandro  Laippala 	81
Ray Kurzweil ùëé 	82
Joohong  Lee 	83
PSD	84
['Self-supervised Learning', 'Data Refinement', 'Machine Translation']	85
Alane  Suhr 	86
Equal contribution	87
AMR 17	88
AMR 15	89
Japanese-Chinese	90
Pearson correlation	91
Adithya  Renduchintala 	92
Jamo Pair Encoding: Subcharacter Representation-based Extreme Korean Vocabulary Compression for Efficient Subword Tokenization	93
Takaaki  Tanaka 	94
Zijian  Yang 	95
Yun-Hsuan Sung ùëé 	96
Boliang  Zhang 	97
Marco  Passarotti 	98
Nikola  Lenci 	99
Multilingual language model pre-training	100
The paper introduces new benchmark datasets for Korean NLI and STS, named KorNLI and KorSTS, respectively. It also establishes baseline models for these datasets.	101
Joakim  Nivre 	102
Difficulty in performing high-quality translation in real time due to syntactic differences and simultaneity requirements.	103
The study proposes a hybrid tokenization approach combining morphological segmentation and Byte Pair Encoding (BPE) for improved performance.	104
Best F1 en-fr	105
Edouard  Grave 	106
Vishrav  Chaudhary 	107
Multilingual Sentence Encoder (USE)	108
Hung-Yu  Kao 	109
Giuseppe G A Bowman 	110
Larraitz  Uria 	111
Sumire  Uematsu 	112
Johns Hopkins University	113
Zeljko  Agic 	114
Aitziber  Atutxa 	115
Marie-Catherine  Connor 	116
Veronika  Krek 	117
Wiki	118
Elena  Pascual 	119
Proceedings of the Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics2020	120
['Quality Estimation', 'Machine Translation', 'Unsupervised Learning']	121
Pearson\s œÅ	122
WMT15	123
Anders  Irimia 	124
Jussi  Piitulainen 	125
Precision	126
WMT 2020 QE Task 1	127
Cenel-Augusto  Perez 	128
Radu  Haug 	129
Existing methods for AMR-to-text generation often rely solely on AMR-annotated data and do not leverage strong pre-trained language models effectively.	130
Zsolt  Sz√°nt√≥ 	131
ParaCrawl v1.0	132
ReQA	133
To solve the difficulty in performing high-quality translation in real time due to syntactic differences and simultaneity requirements, the proposed framework with prefix translation and a stopping criterion is innovatively introduced, achieving balanced translation quality and latency with reasonable BLEU scores.	134
Cross-lingual Chinese-Japanese NLP tasks	135
Noah Constant ùëé 	136
The paper proposes two algorithms that enhance the flexibility of Byte-Pair Encoding tokenizers, reducing the vocabulary budget for Korean in multilingual models.	137
Kiril  Simov 	138
Mart√≠nez  Mareƒçek 	139
RMSE	140
Cross-lingual language understanding	141
NIST	142
Koldo  Goenaga 	143
Manuel  Mager 	144
Modality-transferable model	145
Timothy  Dobrovoljc 	146
Existing quality estimation methods require extensive annotated data and consider the MT system as a black box.	147
['BERT','Collaborative Filtering','Explainable AI']	148
Simon  Kanerva 	149
Shadi  Saleh 	150
IWSLT16	151
Juri  Opitz 	152
UnihanLM, a self-supervised pre-trained masked language model using a two-stage coarse-to-fine training approach leveraging the Unihan database.	153
UnihanLM	154
Training and comparing transformer language models using both BPE and unigram LM methods to assess their morphological alignment and downstream performance.	155
An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks	156
Abrego  ùëé 	157
Multi-modal emotion recognition	158
Iakes  Ginter 	159
BR	160
Rich√°rd  Erjavec 	161
['Transductive Parsing','Universal Decompositional Semantics','Natural Language Understanding']	162
Existing methods struggle to efficiently and accurately rate AMR quality without using costly gold references.	163
University of California	164
KorSTS	165
Md Arafat Sultan 	166
SR	167
Sam  Bosco 	168
Steve Yuan ùëè 	169
WMT2014 (en-de)	170
Hyungjoon  Soh 	171
['multi-modal emotion recognition', 'emotion embeddings', 'cross-modality transfer']	172
To solve the problem of the influence of subword tokenization in pretrained language models, a comparison of BPE and unigram LM methods is innovatively proposed. The results suggest that unigram LM tokenization may lead to a more economical vocabulary allocation and improved downstream task performance. The method shows an 85 F1 score on the SQuAD dataset.'	173
Huawei Noah's Ark Lab	174
Existing multilingual models face limitations in achieving high performance across various languages, particularly low-resource ones, without diluting model capacity.	175
A transition-based parser complemented by an A* parser improves parsing speed significantly while maintaining or improving accuracy.	176
Modality-Transferable Emotion Embeddings for Low-Resource Multimodal Emotion Recognition	177
Best F1 en-de	178
['AMR Quality Estimation','Convolutional Neural Network','Natural Language Processing']	179
UDS1.0	180
Korean NLI and STS	181
National Cheng Kung University	182
Zdenƒõk  ≈Ωabokrtsk√Ω 	183
Daniel Cer ùëé 	184
['Multilingual Quality Estimation','Machine Translation','Zero-Shot Learning']	185
Multilingual Universal Sentence Encoder for Semantic Retrieval	186
Transformer Language Models (TLMs)	187
Daniel  Foster 	188
WMT 2018 German-English	189
Bowen  Liang 	190
Semantic Retrieval	191
NER	192
Dawoon  Jung 	193
To solve the inefficiency in accurately rating AMR quality without gold references, the paper innovatively proposes a lightweight CNN that rates AMR graphs. The method achieves improved accuracy with Pearson\'s œÅ of 0.695, Smatch precision of 0.696, and Smatch recall of 0.719.	194
Daniel  Zeman 	195
KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding	196
A fine-tuned GPT-2 model combined with cycle consistency-based re-scoring approach is proposed to generate text directly from AMR graphs.	197
Zarana  Parekh 	198
GPT-2 with cycle consistency-based re-scoring	199
An unsupervised quality estimation approach based on extracting information from the NMT system‚Äôs uncertainty quantification.	200
Yinfei  Yang 	201
Pearson r	202
Wuhan University	203
Dual-Encoder Models and Transformer-Big	204
AUCPR en-de	205
Alexander  Koller 	206
neural machine translation	207
To solve the problem of brittleness in existing categorical semantic formalisms, a transductive sequence-to-graph parsing model is innovatively proposed to jointly map natural language utterances to UDS graph structures and predict decompositional semantic attributes, achieving 84.19 in F1 Score on the UDS1.0 dataset.	208
Tokyo Institute of Technology ‚Ä†	209
Matthias  Lindemann 	210
ùëé  Law 	211
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations	212
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics	213
To solve the suboptimal performance existing in semantic retrieval, the multilingual sentence embedding models using a multi-task trained dual-encoder are innovatively proposed and the state-of-the-art results on monolingual and cross-lingual semantic retrieval are obtained.	214
Salim  Roukos 	215
IEMOCAP	216
Acc	217
Veronika  Vincze 	218
Universal Decompositional Semantic Parsing	219
Smatch recall	220
Tahira  Naseem 	221
IWSLT2014 (es-en)	222
LDC2017T10	223
Brian Strope ùëé 	224
Lucia  Specia 	225
Dag  Hajiƒç 	226
F1 Score	227
MLQA	228
DiDi Labs	229
Quality Estimation for Machine Translation	230
Qun  Liu 	231
AI Hub	232
Ro-En	233
C»Ét»Élina  Manning 	234
Debiased AMR Quality Data	235
Ram√≥n  Fernandez 	236
Existing review-based recommender systems struggle with static word embeddings and the lack of explainability.	237
Christopher  Lynn 	238
Reut  Tsarfaty 	239
To solve the extensive data requirement and black box problem in quality estimation for machine translation, an unsupervised approach using information from NMT system uncertainty quantification is innovatively proposed and achieves competitive results with high correlation to human judgment.	240
Department of Language Science and Technology	241
ùëé Google AI Mountain View	242
Evaluating the differences between BPE and unigram LM tokenization methods and their impact on pretrained language models.	243
Unsupervised Quality Estimation for Neural Machine Translation	244
Existing AM dependency parsers are slow due to computationally expensive type constraints needed to ensure well-typedness.	245
SQuAD	246
Xin  Jiang 	247
Avg F1	248
Kakao  Brain 	249
['Chinese-Japanese Pre-trained Model','Cross-lingual Language Model','Unihan Database']	250
Machine Translation Quality Estimation	251
Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing	252
Simultaneous neural machine translation	253
Jiyeon  Ham 	254
Avg accuracy	255
Yunhsuan  Sung 	256
Shunsuke  Mori 	257
Findings of the Association for Computational Linguistics: EMNLP 2020	258
The noisy nature of web-crawled parallel corpora for training NMT systems and the sensitivity of NMT models to such noise.	259
AUC	260
Pascale  Fung 	261
Mandy Guo ùëé 	262
Byte Pair Encoding is Suboptimal for Language Model Pretraining	263
Verginica  Missil√§ 	264
AMR Quality Rating with a Lightweight CNN	265
Unsupervised Cross-lingual Representation Learning at Scale	266
Pingpong AI Research	267
Center for Artificial Intelligence Research (CAiRE) Department of Electronic and Computer Engineering	268
A lightweight convolutional neural network (CNN) that transfers AMR graphs to the domain of images to estimate their quality.	269
Yun  Chen 	270
Transductive sequence-to-graph parsing model	271
A General Framework for Adaptation of Neural Machine Translation to Simultaneous Translation	272
Existing categorical semantic formalisms struggle with non-prototypical instances and face difficulties in handling changing label ontologies and new datasets.	273
Mark  Fishel 	274
Tokenization method evaluation	275
Existing noisy pairwise datasets, like parallel texts mined from the internet, require a refined subset to improve learning from weakly supervised signals.	276
Improvement Gain	277
PAS	278
To solve the problem of finding the best tokenization strategy for Korean NLP tasks, a hybrid approach of morphological segmentation followed by BPE is innovatively proposed, achieving BLEU scores of 39.05 and 36.06 for Ko-En translation, and an F1 score of 81.0 for KorQuAD.	279
Wei  Wang 	280
Yo Joong Choe 	281
To solve the limitation of model capacity dilution and low performance in low-resource languages present in cross-lingual language understanding, the innovative XLM-R model is proposed. This method shows improved results across various multilingual benchmarks, including XNLI (80.9% accuracy), MLQA (70.7% F1), and NER (89.43% F1).	282
Parallel Corpus Filtering via Pre-trained Language Models	283
Canwen  Xu 	284
Maria Jesus Aranzabe 	285
Anna  Mcdonald 	286
Elias  Stengel-Eskin 	287
Existing multilingual language models struggle with the large character set vocabulary size required for languages like Korean, increasing cost and complexity.	288
Fr√©d√©ric  Blain 	289
Facebook AI	290
['AMR-to-Text Generation','Pre-trained Language Model','Cycle Consistency']	291
University of Tartu	292
Dept. of Computational Linguistics	293
Shuo  Sun 	294
To solve the reliance on AMR-annotated data alone in AMR-to-text generation tasks, the innovative fine-tuned GPT-2 with cycle consistency-based re-scoring method is proposed and achieves results of 32.32 BLEU and 62.79 chrF++ on the LDC2017T10 dataset.	295
Gustavo  Hernandez 	296
Jan  ≈†tƒõp√°nek 	297
The paper introduces multilingual and zero-shot QE models using pre-trained contextual representations and explores multi-task learning strategies.	298
Guillaume  Wenzek 	299
Zihan  Liu 	300
['XLM-R','Multilingual Language Models','Cross-lingual Transfer']	301
In neural machine translation, current label smoothing methods are heuristic and lack a clear understanding of their optimization and practical application.	302
An Exploratory Study on Multilingual Quality Estimation	303
AMR 20	304
Xiao  Chen 	305
Two self-supervised iterative filtering methods employing dual-encoder models are proposed to denoise and refine datasets by selecting useful subsets without external annotations.	306
Ryan  Matsumoto 	307
Recommendation and rating prediction	308
IWSLT2014 (de-en)	309
En-Zh	310
Universal	311
AUCPR en-fr	312
Ahmed  El-Kishky 	313
DM	314
Francisco  Guzm√°n 	315
Data Refinement for Machine Translation	316
Ilji  Choi 	317
Bruno  Gonzales 	318
IBM Research AI	319
Wikipedia	320
Berta  Goldberg 	321
BLEU	322
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)	323
Gustavo Hern√°ndez √Åbrego 	324
XNLI	325
Kyubyong  Park 	326
Myle  Ott 	327
Elena  Ion 	328
To solve the underutilization of shared morphological features in Chinese and Japanese characters in cross-lingual models, the UnihanLM is innovatively proposed. This model leverages the Unihan database and a two-stage training approach, resulting in substantial improvements in unsupervised machine translation tasks, such as achieving a BLEU score of 44.59 on the ASPEC-JC dataset.	329
['Korean NLI','Korean STS','Natural Language Understanding']	330
Maria  Simi 	331
Kevin  Knight 	332
UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database	333
Aaron  Smith 	334
A novel model (BENEFICT) combining BERT, MLP, and MSP for contextual feature extraction, user-item interaction modeling, and explanation generation.	335
Arantza  De Marn- Effe 	336
Christian  Herold 	337
Seongbo  Jang 	338
Kartikay  Khandelwal 	339
Marina  Fomicheva 	340
Yuji  Ma≈°ek 	341
Google Research	342
Liangyou  Li 	343
Simon- Montemagni  Miyao 	344
Toma≈æ  Dozat 	345
Alexis  Conneau 	346
Lilja  √òvrelid 	347
Aaron Steven White 	348
KorNLI	349
AMR-to-text generation	350
Jonas  Groschwitz 	351
BENEFICT	352
Transition-based parser	353
Lightweight CNN	354
Yingbo  Gao 	355
['Tokenization','Korean NLP','Hybrid approach']	356
['Subword Tokenization', 'Language Models', 'NLP']	357
Ahmad  Amin 	358
['Parallel Corpus Filtering','Machine Translation','Pre-trained Language Models']	359
['Multilingual Sentence Embeddings','Semantic Retrieval','Cross-Lingual Tasks']	360
Natalia  Silveira 	361
Yelp	362
Self-Supervised Learning for Pairwise Data Refinement	363
John  Bauer 	364
Recall	365
To solve the issue of refining noisy pairwise datasets for training from weakly supervised signals, the proposed self-supervised iterative filtering methods effectively produce competitive performance without the need for annotations, achieving significant improvements in parallel text mining tasks.	366
EDS	367
Existing cross-lingual models underutilize the shared morphological features between Chinese and Japanese characters, affecting their performance on NLP tasks.	368
Naoaki  Okazaki 	369
To solve the lack of benchmark datasets for Korean NLI and STS tasks in NLP, the introduction of KorNLI and KorSTS datasets is innovatively proposed, achieving notable results: 82.75% accuracy on KorNLI and 83.00 Spearman correlation on KorSTS.	370
Radu  Florian 	371
Lack of publicly available benchmark datasets for Korean NLI and STS tasks, which hinders the development of Korean NLU models.	372
Fast semantic parsing with well-typedness guarantees	373
Loganathan  Ramasamy 	374
Ahmad  Riyaz 	375
Miriam  Celano 	376
Wenliang  Dai 	377
Parallel corpus filtering for machine translation	378
Lisa  Yankovskaya 	379
David  M»Ér»Énduc 	380
SQuAD v1.0	381
Tao  Ge 	382
Hiroshi  Johannsen 	383
ASPEC-JC	384
F1	385
Luke  Zettlemoyer 	386
A general framework is proposed that includes prefix translation with a consecutive NMT model and a stopping criterion to balance translation quality and latency.	387
OR	388
KorQuAD	389
To solve the problem of noisy web-crawled parallel corpora in machine translation, the innovative approach of using pre-trained language models like multilingual BERT and GPT for filtering is proposed, achieving state-of-the-art results.	390
To solve the heuristic nature and unclear optimization of current label smoothing methods in neural machine translation, an innovative generalized formula and theoretical solution combined with empirical studies are proposed, exploring various hyperparameters for optimal performance.	391
Determining the optimal tokenization strategy for Korean NLP tasks across various domains.	392
Cristina  Bhat 	393
Kaja  Diaz De Ilarraza 	394
Barbara  Plank 	395
Spearman	396
Katalin  Simk√≥ 	397
Rudolf  Rosa 	398
Mojgan  Seraji 	399
Kaj  Bostrom 	400
Amazon	401
F-score	402
Greg  Durrett 	403
Towards a Better Understanding of Label Smoothing in Neural Machine Translation	404
Baseline QE Model (BASE)	405
Nikolaos  Aletras 	406
